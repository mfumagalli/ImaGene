{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification on continuous variables using _ImaGene_\n",
    "\n",
    "In this example, the aim is to estimate a continuous parameter.\n",
    "Please refer to the tutorial for binary and multiclass classification for an in-depth explanation of each step and case study.\n",
    "Briefly, we aim at estimating the selection coefficient on one variant conferring lactase persistence in Europeans.\n",
    "We will discretize the distribution into classes and perform a multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import _pickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pymc3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, activations, optimizers, regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import load_model\n",
    "\n",
    "import itertools\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pydot # optional, but required by keras to plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../ImaGene.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data from VCF file and store it into _ImaGene_ objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_LCT = ImaFile(nr_samples=198, VCF_file_name='LCT.CEU.vcf');\n",
    "gene_LCT = file_LCT.read_VCF();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration, here we will sort rows by distance and columns by frequency and resize the image to (128,128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_LCT.filter_freq(0.005);\n",
    "gene_LCT.sort('rows_dist');\n",
    "gene_LCT.sort('cols_freq');\n",
    "gene_LCT.resize((128,128));\n",
    "gene_LCT.convert(flip=True);\n",
    "gene_LCT.plot();\n",
    "gene_LCT.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run and process simulations to be used for training the neural network\n",
    "\n",
    "We provide an example of parameter file called `params_continuous` which simulates a total of 205,000 loci of 80kbp with allelic selection coefficients from 0 to 400 in $2N_e$ units with $N_e=10,000$ with a step of 1 and additive effect.\n",
    "All other parameters are set as in the example of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change accordingly, e.g.:\n",
    "# path_sim = '/home/mfumagal/Data/ImaGene/Tutorials/' # for my local machine\n",
    "# path_sim = '/mnt/quobyte/ImaGene/' # for workshop spp1819\n",
    "path_sim = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit `params_continuous.txt` file accordingly. Here I assume that simulations will be stored in `path+Continuous/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wish to generate new training data, do not run otherwise\n",
    "import subprocess\n",
    "subprocess.call(\"bash ../generate_dataset.sh params_continuous.txt\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to perform a multiclass classification to estimate the selection coefficient, a continuous parameter. \n",
    "In _ImaGene_ we can easily do that by imposing a new discrete set of classes and reassign the new targets to such classes with the methods `.set_classes` and `.set_targets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement, train and evaluate the neural network\n",
    "\n",
    "The pipeline for training and testing is the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i <= 10:\n",
    "\n",
    "    # simulations \n",
    "    file_sim = ImaFile(simulations_folder=path_sim+'Continuous/Simulations' + str(i), nr_samples=198, model_name='Marth-3epoch-CEU')\n",
    "\n",
    "    # retain only 20 data points per class as a quick example\n",
    "    gene_sim = file_sim.read_simulations(parameter_name='selection_coeff_hetero', max_nrepl=20)\n",
    "    \n",
    "    # manipulate data\n",
    "    gene_sim.filter_freq(0.005)\n",
    "    gene_sim.sort('rows_dist')\n",
    "    gene_sim.sort('cols_freq')\n",
    "    gene_sim.resize((128,128))\n",
    "    gene_sim.convert(flip=True)\n",
    "    \n",
    "    # we assign 11 classes out of all the data simulated\n",
    "    gene_sim.set_classes(nr_classes=11)\n",
    "    if i == 1:\n",
    "        print(gene_sim.classes)\n",
    "    # and we assign targets corresponding to the previously set classes \n",
    "    gene_sim.set_targets()\n",
    "    \n",
    "    # randomise data\n",
    "    gene_sim.subset(get_index_random(gene_sim))\n",
    "\n",
    "    # targets have to be converted into categorical data; \n",
    "    # here we can use some extra options to, for instance, impose a Guassian distribution on the true targets\n",
    "    gene_sim.targets = to_categorical(gene_sim.targets, wiggle=0, sd=0.5)\n",
    "    \n",
    "    # at first iteration we build the model \n",
    "    # note that, as an illustration, we don't implement a final fully-connected layer as we are double sorting the matrix\n",
    "    if i == 1:\n",
    "\n",
    "        model = models.Sequential([\n",
    "                    layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid', input_shape=gene_sim.data.shape[1:]),\n",
    "                    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                    layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid'),\n",
    "                    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                    layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid'),\n",
    "                    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "                    layers.Flatten(),\n",
    "                    layers.Dense(units=len(gene_sim.classes), activation='softmax')])\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        net_LCT = ImaNet(name='[C32+P]+[C64+P]+[C128+P]')\n",
    "\n",
    "    # training for iterations from 1 to 9\n",
    "    print(i)\n",
    "    if i < 10:\n",
    "        score = model.fit(gene_sim.data, gene_sim.targets, batch_size=32, epochs=1, verbose=1, validation_split=0.10)\n",
    "        net_LCT.update_scores(score)\n",
    "    else:\n",
    "        # testing for iteration 10\n",
    "        net_LCT.test = model.evaluate(gene_sim.data, gene_sim.targets, batch_size=None, verbose=1)\n",
    "        net_LCT.predict(gene_sim, model)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory where to save models, e.g. \n",
    "# path='/home/mfumagal/Data/ImaGene/Tutorials/Data/' # my local machine\n",
    "# path='./' # for workshop spp1819\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final (trained) model\n",
    "model.save(path+'model.multi_cont.h5')\n",
    "\n",
    "# save testing data\n",
    "gene_sim.save(path+'gene_sim.multi_cont')\n",
    "\n",
    "# save network\n",
    "net_LCT.save(path+'net_LCT.multi_cont')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to load all these files you can use the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sim = load_imagene(path+'gene_sim.multi_cont');\n",
    "net_LCT = load_imanet(path+'net_LCT.multi_cont');\n",
    "model = load_model(path+'model.multi_cont.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the training\n",
    "net_LCT.plot_train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can report loss, accuracy and confusion matrix as any classification task although in this case it may be more informative to investigate the difference between true and predicted values instead of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the testing results [loss, accuracy] and plot confusion matrix\n",
    "print(net_LCT.test)\n",
    "net_LCT.plot_cm(gene_sim.classes, text=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the trained network on your genomic data of interest\n",
    "\n",
    "Finally, we use the trained network to estimate the selection coefficient of our locus of interest.\n",
    "A plot of the probability distrbution of selection coefficient can be obtained by, for instance, drawing MCMC samples. MCMC samples can also be used to obtain Bayes Factors and HPDI. \n",
    "(However, it is not guaranteed that this approach is better than using a regression as final layer. More tests need to be conducted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = plot_scores(model, gene_LCT, classes=gene_sim.classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In output this function returns the following values: MAP, MLE, HPD, BF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
